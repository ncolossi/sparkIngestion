{"cells":[{"cell_type":"markdown","source":["### Consume data from Event Hubs"],"metadata":{}},{"cell_type":"markdown","source":["Setup variables for this job"],"metadata":{}},{"cell_type":"code","source":["# make sure to use your Event Hubs connection string to the event hubs created (not the namespace level)\neh_connection = \"put your event hubs connection here\"   # or use dbutils.secrets.get(scope=\"your scope\",key=\"your scope key to the eventhubs connection\")\n\nlake = \"/mnt/lake/raw/\"\ntableName = \"raw.events\"\ndeltaDataPath = lake+tableName\ncheckpointPath = \"/checkpoint/\"+tableName"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Clean up checkpoint and table data (to start with a clean environment)"],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.rm(checkpointPath, recurse=True)\ndbutils.fs.rm(deltaDataPath, recurse=True)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Create the Structured streaming job"],"metadata":{}},{"cell_type":"code","source":["import json\n\n# event hubs connection and params\nehConf = {'eventhubs.connectionString' : eh_connection}\nstartingEventPosition = {\n  #\"offset\": \"@latest\",   \n  \"offset\": \"-1\",\n  \"seqNo\": -1,            \n  \"enqueuedTime\": None,   \n  \"isInclusive\": False\n}\nehConf[\"eventhubs.startingPosition\"] = json.dumps(startingEventPosition)\nehConf[\"maxEventsPerTrigger\"] = 100000\n\n# Start streaming\nstreamingInputDF = (spark.readStream\n    .format(\"eventhubs\")\n    .options(**ehConf)\n    .load())"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Parse input json data and save into a delta table"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType\nfrom pyspark.sql.functions import substring,col,from_json\n\ninputSchema = StructType([\n  StructField(\"messageId\", LongType(), True),\n  StructField(\"deviceId\", IntegerType(), True),\n  StructField(\"temperature\", IntegerType(), True),\n  StructField(\"genTimestamp\", StringType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["query = (streamingInputDF\n  .select(from_json(col(\"body\").cast(\"string\"), inputSchema).alias(\"value\"))\n  .selectExpr(\"value.*\")\n  .withColumn(\"genDate\", substring(\"genTimestamp\", 1, 10))\n  .writeStream\n  .format(\"delta\")\n  .partitionBy(\"genDate\")\n  .outputMode(\"append\")\n  .trigger(once=True)\n  #.trigger(processingTime='30 seconds')\n  .option(\"checkpointLocation\", checkpointPath)\n  .start(deltaDataPath) )"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Wait so streaming starts and create a table definition mapping to location"],"metadata":{}},{"cell_type":"code","source":["spark.sql(f\"CREATE DATABASE IF NOT EXISTS raw\")\nspark.sql(f\"DROP TABLE IF EXISTS {tableName}\")\nspark.sql(f\"CREATE TABLE {tableName} USING DELTA LOCATION '{deltaDataPath}'\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Check that data is coming in"],"metadata":{}},{"cell_type":"code","source":["display(sql(f\"select deviceId, count(*) from {tableName} group by deviceId\"))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["Check the folder for this table in ADLS gen2\n\nThen optimize to see compaction take place"],"metadata":{}},{"cell_type":"code","source":["# Optimize table, and clean up snapshots\nspark.sql(f\"OPTIMIZE {tableName}\")\nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)\nspark.sql(f\"VACUUM {tableName} RETAIN 0 HOURS\")    # 0 HOURS is risky if streaming is running"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Check how delta table keeps history of changes in the table"],"metadata":{}},{"cell_type":"code","source":["display(spark.sql(f\"DESCRIBE HISTORY {tableName}\"))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":19}],"metadata":{"name":"02-consume-data","notebookId":2035083227368876},"nbformat":4,"nbformat_minor":0}